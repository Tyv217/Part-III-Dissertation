\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Introduction}
\label{firstcontentpage}

\section{Concept BottleNeck Models}
Machine Learning (ML) models are universal approximation solutions to problems
and have been viewed and used traditionally as "black-box" solutions, 
where users simply query the model and receive an answer to the problem without 
necessarily knowing the exact reasoning process behind it. Over the past decade,
the increased application of ML models along with this "black-box" property has
raised many concerns, especially in areas where decisions made are critical to
human safety, such as in medicine or automated driving. To increase
interpretability, researchers developed Concept Bottleneck Models (CBMs)~\cite{cbm} that 
predicts a set of human-interpretable concepts from input, and then uses the
predicted concepts to predict labels. This increases the interpretability of the model 
as humans
can understand the basis of the ML model predictions via the high-level 
intermediate concepts that the model is trained to predict. 

Another additional benefit of these models is that when used in 
practice, experts can intervene in the predicted concepts to the correct concepts to 
generate more accurate model output.
Given that experts have limited time, determining 
the order of concepts to query experts to intervene on, 
to maximize the accuracy of the model given a limited budget, becomes an important problem.
Since the objective behind developing these models is to
use them in real life, this project utilizes different
datasets that reflect different situations in real life
to measure the performance. 
This is further discussed in Section~\ref{}.

\section{CEM and IntCEM}
Current research has made significant progress on optimizing these models for interventions,
including numerous studies on developing models to learn
the optimal order of concepts to intervene on~\cite{coop},
most notably Intervention-aware Concept Embedding Models (IntCEMs)~\cite{intcem}. IntCEMs build upon 
Concept Embedding Models~\cite{cem}, a variant of CBMs that utilize 
embeddings to represent the intermediate concepts such that models
learn to encode information about unlabelled concepts while still preserving
the valuable interpretability of CBMs. IntCEMs augment CEMs with an additional
model that learns to predict the next concept to intervene on given the current state
of the CBM, which is also used during training to increase the CBM's
sensitivity to interventions. IntCEMs achieve state-of-the-art performance on
the performance of interventions while still maintaining similar performance when
no interventions are performed.

\section{Reinforcement Learning}
Despite the above-mentioned improvements, there is still a big gap between the
performance
of these intervention policies versus the best possible performance, i.e. the performance of 
intervention policies that have access to the ground truth output labels.
Additionally, existing approaches such as IntCEM are greedy, which means that
they learn to predict concepts that maximize the performance at each step.
it has been speculated that non-greedy methods may outperform 
these existing greedy methods, with the objective being maximizing 
performance after intervening a certain number of concepts rather than maximizing 
performance at each step. 
One such approach that may be used to generate non-greedy intervention policies 
is Reinforcement Learning (RL)~\cite{rl}.

This project focuses specifically on trying to solve the 
question of finding a good intervention policy for a given 
budget using RL and Surrogate models to model conditional 
probabilities to guide the RL model, taking inspiration
from an approach~\cite{gsmrl} in a similar setting of
Active Feature Acquisition (AFA)~\cite{afa}.
This project successfully develops a novel RL-based method
that when combined with
existing methods from IntCEM to increase sensitivity
to interventions, is able to learn an intervention policy
and a Concept BottleNeck Model that outperforms
existing 
non-greedy intervention policies and models
for different budgets, while maintaining similar performance
when no interventions are performed.

\end{document}