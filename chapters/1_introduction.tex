\csname documentclass\endcsname[../main.tex]{subfiles}
\begin{document}
\chapter{Introduction}
\label{firstcontentpage}
% write more about motivating things before jumping into solution 
% paragraph above to sum up: same as contribution -> can be left out or merged


% open problem
Regular supervised Machine Learning (ML) models learn to 
predict the labels of inputs.
Concept Bottleneck Models (CBMs)~\cite{cbm}and Concept Embedding Models (CEMs)~\cite{cem} are 
ML models designed to increase the interpretability of model predictions by decomposing a model into
two submodels, splitting the original process into predicting a set of human-interpretable
concepts / features present in the input, then predicting the label using these concepts.
This allows us to understand the reasoning behind the predications better, 
mitigating some of the downsides associated with using ML models as ``black-box'' models,
especially in areas where 
decisions made can be critical to 
human safety, such as medicine
and criminal justice.
During inference time, professionals can intervene on CBMs by correcting
the predicted concepts, leading to more accurate predicted labels. 
This leads to the problem of determining the concepts to intervene on in order
to achieve the best model performance.

% why it is important and why we should care to solve this (motivation)
Previous studies have shown that the choice of concepts to 
intervene on can have significant impacts on the accuracy of the model~\cite{coop, intcem},
and thus it is important to find a good policy to determine what concepts
we should intervene on.
Due to the costs
associated with performing such an intervention, we introduce budgets,
which add constraints on the number of interventions allowed. 
This
reflects real-life scenarios where
there exists limits on the number of times we 
can query an expert to intervene on concepts.

% your research question(s)
The problem then becomes finding policies that can determine the best 
concepts to perform interventions on for a given budget. In particular,
we investigate 
the research question of 
finding a non-greedy policy to determine
the set of concepts to intervene on for a budget.
We hypothesize that non-greedy policies
yield more optimal solutions as they maximize 
the model accuracy after a number of interventions rather than at each step.

% what other have done and how their work is deficient with regards to your question(s)
Past methods have mainly focused on greedy approaches, which
includes heuristic-based approaches such as intervening on concepts that minimize the
model's uncertainty~\cite{coop}, or ML-based approaches that learn a greedy policy
like in IntCEMs~\cite{intcem}. Additionally existing approaches either 
attempt to model the distribution of concepts and use that
to sample interventions~\cite{energycem}, 
or directly learn a policy~\cite{intcem}, while limited
work has been done on combining the capabilities of models 
from these two approaches.

% The key technical idea (not abstract but technical detail) and innovation that you did that enabled you to answer your question
To find a non-greedy policy, we investigate using Reinforcement Learning (RL)~\cite{rl}
to learn a policy that maximizes
the model accuracy after a set number of interventions,
using the accuracy of the CBM as a reward to train the RL agent.
In order
to guide the agent throughout the intervention process,
we use surrogate models that model the distribution of concepts using
a variant of normalizing flow models. These models learn a latent distribution
and an invertible transformation to model the distribution of the concepts.
This allows us to compute the conditional likelihood of concepts 
and use as an intermediate reward to the RL agent. Additionally we can sample
from the underlying distribution to obtain concepts with the highest likelihood,
which provides auxiliary information to the agent.

% How you did it (methodology)
After pretraining a surrogate model to learn the distribution of concepts,
we train a Reinforcement Learning agent in conjunction with a CEM, which we name RLCEM, to learn a 
non-greedy intervention policy, using
the expected information gain from the surrogate model
as a reward, and a final reward based on the 
accuracy of the CBM after all interventions.
During training,
the agent learns a policy to select
the optimal concepts for interventions for different budgets,
whereas the CBM is also trained on the sampled interventions
to increase its sensitivity to interventions determined by 
the learnt policy.

% What the results were
To evaluate this approach, we train RLCEM on two different datasets
and compare its performance to IntCEM, the state-of-the-art 
model for intervention performance that learns a greedy policy 
for interventions.
% We show that this approach can outperform existing greedy policies
% for different budgets, achieving better model performance
% after a set number of interventions and reaching similar
% model performance with 25\% less interventions. 


% % What impact could your contributions have
To sum up, this project has the following contributions:
\begin{itemize}
    \item Introduce budgets into the problem setting and shown that non-greedy policies can outperform greedy policies.
    \item Develop and train Surrogate models by adapting AC Flow models to be used to model conditional concept likelihoods.
    \item Train RL Concept Embedding Models that utilize Reinforcement Learning
    that can learn a non-greedy intervention policy
    with better intervention performance than existing methods, achieving better performance
    with up to 25\% less interventions. At the same time the model maintains similar performance under 
the absence of interventions. 
    \item Standardize the RL environment of RLCEMs inheriting from the Gymnasium Interface~\cite{gymnasium} which
    can be used in future research in RL-based intervention policies.
\end{itemize}

However, we note that
this approach also has its drawbacks, including
higher time complexity, and its performance is very dependent on 
the nature of the task and concepts, which affects the robustness of the method.

% \textbf{Terminology}
% The term ``concepts'' hereafter strictly refers to the labelled concepts present
% in the annotated dataset
% that are used to make label predictions
% . For example, in a dataset of identifying birds,
% these concepts include ``has a red beak'', ``has white feathers'', etc.
% Other similar terms such as ``features'', ``attributes'', do not refer to the same thing
% and will not be used interchangeably with
% "concepts''.

% successfully develops a novel approach in using
% Reinforcement Learning to learn a non-greedy intervention policy 
% for different budgets,
% where we successfully show that our RL-based approach
% learns non-greedy policies that can outperform existing
% greedy policies for different budgets, 

%  We model the costs
% associated with interventions and budgets for interventions, which are constraints present in
%  real life
% applications, and set the main research question to be
% determining the concepts to intervene on for different budgets for a CEM
% to maximize its performance.

% To answer the above question, we first investigate the differences between greedy and non-greedy policies,
% proving that non-greedy algorithms can outperform
% their greedy counterparts. We then build a Reinforcement Learning agent
% that learns to select the next concept to intervene in order to maximize the model's performance for a given budget.
% To guide the agent throughout the intervention process, we utilize surrogate models that model
% the conditional distribution of concepts. A detailed description of these models are in Section~\ref{method:rlcem}.
% These surrogate models guide the RL agent throughout the intervention
% process by rewarding the agent for intervening on concepts that lead to higher probabilities of
% intervened concepts, and provide auxiliary information about the unintervened concepts
%  in order for the agent to make more informed decisions.
% % The output of these surrogate models are then used by a Reinforcement Learning agent in order to
% % perform sequential intervention decisions to maximize the final performance, which is trained
% % in conjunction with a CBM in order to increase the sensitivity of the CBM to
% % the interventions. 
% We augment CEMs with this RL agent and a pretrained surrogate model to form RLCEM, 
% and train the CEM and the RL agent simultaneously, which helps learn a non-greedy policy
% and increase the sensitivity of the CEM to these interventions.

% The results show that such RLCEMs
% outperform existing models when intervened using the learnt policy, while achieving 
% similar performance
% under the absence of interventions. This project successfully demonstrates the how a non-greedy 
% policy can be learnt using Reinforcement Learning and surrogate models that 
% outperforms existing greedy policies for different budgets, showing that Reinforcement Learning
% is a viable approach for learning more optimal intervention policies.

\end{document}