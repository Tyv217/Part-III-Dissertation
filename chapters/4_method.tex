\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Design and implementation}


\section{Intervention Policies}

A key advantage of using CBMs is having access to 
run-time interventions, which is the idea of utilizing professionals
to modify incorrect concept predictions to improve the 
performance of the model.
For simplicity, in this project we do not consider incorrect interventions, 
i.e. when the professionals misjudge and modify
 the predicted concepts to incorrect values,
and assume that
all interventions are correct. Thus an intervention
can be defined as the following function, where
the predicted concepts $\hat{\mathbf{c}}$ and the true concepts $\mathbf{c}$ are interpolated
using a multi-hot encoding intervention vector $\bm{\mu}$.

\[I(\hat{\mathbf{c}}, \mathbf{c}, \bm{\mu}) = 
\bm{\mu} \; \mathbf{c} + (1 - \bm{\mu}) \; \hat{\mathbf{c}} \qquad \hat{\mathbf{c}}, \mathbf{c}, \bm{\mu} \in \{0, 1\}^k\]

To formalize an intervention policy for this
project, we define an intervention policy $\mathcal{P}$ to be a policy, either learnt
or heuristic-based, that determines the order of concepts to intervene 
on with the goal of maximizing the accuracy of the $\mathbf{c} \to \mathbf{y}$ concept prediction model.
A greedy intervention policy is thus a collection of functions $\mathcal{P}_i$, each
of which outputs the concept to intervene on at step $i$. The policy aims to minimizes 
the following:

\[\hat{\mathcal{P}} = \bigcup_{i=1}^k \mathop{\mathrm{argmin}}_{\mathcal{P}_i} L_{\text{task}}(\hat{g}(\hat{\mathbf{c}}_{\mathcal{P}_j}), \mathbf{y}) \]
% \[\hat{\mathcal{P}} = \mathop{\mathrm{argmin}}_{\mathcal{P}} \sum_{j = 1}^{k} L_{\text{task}}(\hat{g}(\hat{\mathbf{c}}_{\mathcal{P}, j}), \mathbf{y}) \]
\[\hat{\mathbf{c}}_{\mathcal{P}_0} = \hat{\mathbf{c}}, \hat{\mathbf{c}}_{\mathcal{P}_j} = I(\hat{\mathbf{c}}_{\mathcal{P}_{j-1}}, \mathbf{c}, \mathcal{P}_j(\hat{\mathbf{c}}_{\mathcal{P}_{j-1}}))\]
Which minimizes the loss at each step $j$ sequentially
for all $k$ concepts. At each
step, $\hat{\mathbf{c}}_{j-1}$ is 
the concept after the previous $j-1$ interventions,
Similar to above, the task loss $L_{\text{task}}$ is used to minimize
the discrepancy of $\hat{g}(\hat{\mathbf{c}}_{\mathcal{P}_j})$, 
the output of the label predictor model on the intervened concepts,
and label $\mathbf{y}$.

\subsection{Non-greedy Intervention Policies}

Compared to a greedy intervention policy, a non-greedy intervention 
policy outputs a set of concept to intervene on for a given budget $j$,
which we want to maximize the performance of the 
label predictor model on. This is equivalent to maximizing the following 
function:
\[\hat{\mathcal{P}} = \mathop{\mathrm{argmin}}_{\mathcal{P}} \sum_{j=1}^k L_{\text{task}}(\hat{g}(\hat{\mathbf{c}}_{\mathcal{P}_j}), \mathbf{y}) \]
\[\hat{\mathbf{c}}_{\mathcal{P}_j} = I(\hat{\mathbf{c}}, \mathbf{c}, \mathcal{P}(\hat{\mathbf{c}}, j))\]

Note that the notion of a budget, defined as the number
of concepts allowed to intervene on for simplicity, is only
important for non-greedy policies. Non-greedy policies aim
to maximize the accuracy of the $\mathbf{c} \to \mathbf{y}$ model after using up the intervention budget,
and may select different sets of intervention concepts 
for different budgets. Greedy policies always select the same
concepts per step and thus the budget does not 
affect the concept selected by the policy.

% Budget

\section{Surrogate Models}\label{method:surrogate}

We use surrogate models to model the probabilities of concepts which are used
to guide the RL model. Following Li et al.~\cite{afa} which uses Reinforcement Learning
in the similar problem of Active Feature Acquisition, we use a surrogate model to model
the conditional probabilities $p(\mathbf{x}_u \mid \mathbf{x}_o, \mathbf{y})$, 
where $\mathbf{x}_u$ is the set of unseen concepts, $\mathbf{x}_o$ is the set of seen concepts,
and $\mathbf{y}$ is the label. We select a variant of the popular normalizing flow models~\cite{normalizing-flows},
namely Arbitrary Conditional Flow (AC Flow)~\cite{acflow}
models as our surrogate models, 
which are flow models augmented with the power to model arbitrary conditional probabilities.

As described in Section~ref{background:flow}, normalizing flow models 


Flow models define transformations
using ML models with this property with learnable parameters

Each of these transformations give a normalized
density, and they can be composed to create more complex invertible distributions.
This property allows us to define normalizing flow models which learn a series of transformations
that use a simple latent distribution to model a complex distribution, such as 
the distribution of concepts within a dataset for CEMs.

Due to the invertible property of the transformation,
a model learnt this way also allows us to sample from the complex distribution
according to the underlying probability distribution, which is useful as
this provide information on which concepts are likely to be present given the current seen concepts.
For remaining concepts $x_r$, we obtain $p(x_r \mid x_u, x_o, y)$ by sampling from the model, 
and this information is useful for the RL agent, as for a given set of concepts outputted by 
the $\mathbf{x} \to \mathbf{c}$ model, this information tells the agent which concepts are likely
to be predicted correctly and which are not based on the learnt distribution of concepts given the
predicted label. This allows the RL agent to learn to intervene on concepts that are 
more likely to be predicted incorrectly and improve the accuracy of the model.



\section{RLCEM}\label{method:rlcem}

% Talk about how the RLCEM model is formed
% The goal, losses etc
% Detailed description of what happens during training and testing
% Diagram
% Talk about design choices: num_rollouts, batch_size sampling of budget etc

\subsection{Reinforcement Learning}

We model the problem of finding a non-greedy intervention policy as a 
Reinforcement Learning problem. As mentioned in Section~\cite{background:rl},
Reinforcement Learning are used to find non-greedy solutions to problems
by design as it models the long-term effects of its actions, and aims to 
maximize the overall reward gain. 

In order to formulate the problem of finding an optimal non-greedy intervention policy
as a Reinforcement Learning problem, we model an intervention
trajectory as a Markov Decision Problem~\cite{rl-mdp} according
to the following definition:
\begin{itemize}
    \item States are the observations of the model, including all the information that the is available
    at each step. This includes the state of the CEM, including its bottleneck and predicted concepts,
    the output of the surrogate model, including $p(x_u \mid x_o, y)$, $p(x_r \mid x_u, x_o, y)$,
    $p(x_r \mid x_u, x_o)$ as described in Section~\ref{method:surrogate}.
\end{itemize}

\section{Models and Datasets}
% Brief description of each of these datasets and what they are each used for.
% Leave full description to be in appendix.
% Talk about concept groups
% Talk about subsampling

\end{document}