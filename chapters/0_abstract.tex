\csname documentclass\endcsname[../main.tex]{subfiles}
% Dirty hack to always make compiler 
% compile 
\begin{document}
\chapter*{Abstract}




% open problem
Concept Bottleneck Models (CBMs) are designed for high interpretability by predicting 
human-interpretable concepts from their inputs,
then predicting labels from these concepts. 
These concepts are more meaningful to
experts and easier to work with, where experts
can intervene on the 
mispredicted concepts
by correcting them to improve the accuracy of the predicted label. Previous 
studies have shown that the choice of intervention concepts 
can significantly impact the effects 
the interventions have on the CBM's accuracy.
% and many efforts have been 
% made to develop methods to determine the best order of concepts to intervene on.
% why it is important and why we should care to solve this (motivation)
Since experts have limited time, it is important to develop ways to
determine the best concepts to intervene on to maximise the 
CBM's performance,
% with minimal intervention costs.
% while minimising the intervention costs.
under a budget that limits the number 
of interventions.
% Given a budget
% for interventions, we want to develop methods that achieve the best model performance
% using a limited number of interventions.
Past methods in finding concepts to intervene on
all find a policy
that maximises the effects of these interventions
at each step, making these policies greedy.
% your research question(s)
In this dissertation,
we investigate methods that learn a non-greedy policy 
to determine
concepts to intervene on
for a budget, to maximise the CBM's accuracy.
% what other have done and how their work is deficient with regards to your question(s)

Existing methods
are limited in that they only 
find greedy policies that
maximise some metric,
such as the CBM's predictive accuracy
at each step.
They do not incorporate the notion of intervention budgets,
which we believe is an important issue for real-life applications
of CBMs 
due to the high costs in querying experts for interventions.
We hypothesise that non-greedy policies can find more optimal intervention concepts for different
budgets,
as they maximise the final performance after several interventions rather than at each step.
% The key technical idea (not abstract but technical detail) and innovation that you did that enabled you to answer your question
To learn a non-greedy policy, we apply Reinforcement 
Learning to learn a policy
that maximises the final CBM performance
after a set number of interventions.
We use a surrogate model to model the distribution of concepts to provide intermediate rewards and auxiliary information to the
Reinforcement Learning agent.
% How you did it (methodology)
Using a pre-trained surrogate model, a Reinforcement Learning agent 
% is trained in conjunction with a CBM, 
% which 
learns a policy for determining the concepts
to intervene on for a given budget.
% What the results were

We first show that non-greedy policies can outperform 
greedy policies for different budgets.
We then show that we can learn a non-greedy policy using Reinforcement Learning that outperforms
existing greedy policies for different intervention budgets, achieving 
similar intervention performance with up to 25\% less interventions.
Simultaneously, the model maintains a similar 
performance under the absence of interventions.
% What impact could your contributions have
This shows that Reinforcement Learning can
 learn intervention policies more optimal than
existing approaches for different budgets, 
opening further research in non-greedy intervention policies.
% With the recent developments in Concept Bottleneck Models,
% finding optimal interventions for these models
% to maximize their performance under a given budget is an important issue, as it allows
% us to maximize the performance of these interpretable models while minimizing
% the costs related to expert interventions. This project investigates solving this problem
% using a new approach that combines Reinforcement Learning with surrogate models, with the goal
% of maximizing performance using intervention policies learnt by this new method.

% Compared to 
% previous approaches that only use greedy policies, 
% such as heuristic-based methods or learning a Machine Learning model for a greedy policy,
% the main idea of this project
% is to use Reinforcement Learning to learn a non-greedy policy to maximize the performance
% gain from interventions over a budget rather than at each step. This is done by modelling the problem 
% of deciding
% what concept to intervene on as a Reinforcement Learning problem to maximize the final performance,
% using surrogate models that model the distribution of concepts to provide intermediate rewards and 
% auxiliary information. 
% When used in conjunction
% with methods to increase sensitivity of models to interventions, this project successfully develops a novel
% RL-based method that outperforms existing greedy intervention policies for different budgets, while maintaining
% similar un-intervened performance. 

% This project proposes the idea of learning non-greedy
% intervention policies for Concept Bottleneck Models and incorporates budgets into the problem setting, 
% and shows that
% different budgets can have different optimal intervention trajectories where
% non-greedy policies outperform greedy policies.
% This project also successfully develops a novel approach to learning intervention policies using
% Reinforcement Learning and demonstrate that this approach generates 
% more optimal intervention policies
% that lead to better performance
% compared to existing greedy methods.
\end{document}